{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Imports"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 73,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "from torch import nn\n",
                "import networkx as nx\n",
                "import matplotlib.pyplot as plt"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Medusa Heads"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 74,
            "metadata": {},
            "outputs": [],
            "source": [
                "class BaseModelConfig:\n",
                "    \"\"\"\n",
                "    Configuration class for the Base Model.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, hidden_size=4096, vocab_size=32000):\n",
                "        self.hidden_size = hidden_size\n",
                "        self.vocab_size = vocab_size\n",
                "        self.tokenizer = None\n",
                "\n",
                "\n",
                "class MedusaConfig(BaseModelConfig):\n",
                "    \"\"\"\n",
                "    Configuration class for the Medusa model.\n",
                "\n",
                "    Args:\n",
                "        num_medusa_heads (int): The number of medusa heads.\n",
                "        num_medusa_layers (int): The number of layers in the medusa head.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, num_medusa_heads=5, num_medusa_layers=1, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "        self.num_medusa_heads = num_medusa_heads\n",
                "        self.num_medusa_layers = num_medusa_layers\n",
                "        \n",
                "\n",
                "class ResBlock(nn.Module):\n",
                "    \"\"\"\n",
                "    A Residual Block module.\n",
                "\n",
                "    This module performs a linear transformation followed by a SiLU activation,\n",
                "    and then adds the result to the original input, creating a residual connection.\n",
                "\n",
                "    Args:\n",
                "        hidden_size (int): The size of the hidden layers in the block.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, hidden_size):\n",
                "        super().__init__()\n",
                "        self.linear = nn.Linear(hidden_size, hidden_size)\n",
                "        # Initialize as an identity mapping\n",
                "        torch.nn.init.zeros_(self.linear.weight)\n",
                "        # Use SiLU activation to keep consistent with the Llama model\n",
                "        self.act = nn.SiLU()\n",
                "\n",
                "    def forward(self, x):\n",
                "        \"\"\"\n",
                "        Forward pass of the ResBlock.\n",
                "\n",
                "        Args:\n",
                "            x (torch.Tensor): Input tensor.\n",
                "\n",
                "        Returns:\n",
                "            torch.Tensor: Output after the residual connection and activation.\n",
                "        \"\"\"\n",
                "        return x + self.act(self.linear(x))\n",
                "    \n",
                "class Medusa(nn.Module):\n",
                "    \"\"\"\n",
                "    The head of the Medusa model.\n",
                "\n",
                "    This module is responsible for the final output of the model, which is a single scalar value.\n",
                "\n",
                "    Args:\n",
                "        hidden_size (int): The size of the hidden layers in the model.\n",
                "    \"\"\"\n",
                "\n",
                "    def __init__(self, config):\n",
                "        super().__init__()\n",
                "        self.vocab_size = config.vocab_size\n",
                "        self.hidden_size = config.hidden_size\n",
                "        self.num_medusa_heads = config.num_medusa_heads\n",
                "        self.num_medusa_layers = config.num_medusa_layers\n",
                "\n",
                "        self.medusa_heads = nn.ModuleList(\n",
                "            [\n",
                "                nn.Sequential(    \n",
                "                    *([ResBlock(self.hidden_size)]*self.num_medusa_layers),\n",
                "                    nn.Linear(self.hidden_size, self.vocab_size, bias=False)\n",
                "                ) for _ in range(self.num_medusa_heads)\n",
                "            ]\n",
                "        )\n",
                "\n",
                "    def forward(self, x):\n",
                "        \"\"\"\n",
                "        Forward pass of the MedusaHead.\n",
                "\n",
                "        Args:\n",
                "            x (torch.Tensor): Input tensor.\n",
                "\n",
                "        Returns:\n",
                "            torch.Tensor: Output of the model.\n",
                "        \"\"\"\n",
                "        # Forward pass through base model then through each head\n",
                "        # ignoring the base model for now\n",
                "        return torch.stack([head(x) for head in self.medusa_heads])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Example usage --> feeding the last predicted token from base model\n",
                "# Output from llama model B X pred_seq_len X hidden_size\n",
                "hidden_size = 4096\n",
                "llama_output = torch.randn(1, 10, hidden_size)\n",
                "\n",
                "# Create a Medusa model\n",
                "config = MedusaConfig(hidden_size=hidden_size)\n",
                "medusa = Medusa(config)\n",
                "# Initialize Weights\n",
                "medusa_head_state_dict = torch.load('localpath../medusa_lm_head.pt') # huggingface weights --> FasterDecoding/medusa-vicuna-7b-v1.3/medusa_lm_head.pt\n",
                "medusa.medusa_heads.load_state_dict(medusa_head_state_dict)\n",
                "\n",
                "# Output from Medusa model num_medusa_heads X B X pred_seq_len X vocab_size\n",
                "output = medusa(llama_output)\n",
                "output.shape"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Medusa Buffer & Decoding"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 60,
            "metadata": {},
            "outputs": [],
            "source": [
                "mc_sim_7b_63 = [[0], [0, 0], [1], [0, 1], [2], [0, 0, 0], [1, 0], [0, 2], [3], [0, 3], [4], [0, 4], [2, 0], [0, 5], [0, 0, 1], [5], [0, 6], [6], [0, 7], [0, 1, 0], [1, 1], [7], [0, 8], [0, 0, 2], [3, 0], [0, 9], [8], [9], [1, 0, 0], [0, 2, 0], [1, 2], [0, 0, 3], [4, 0], [2, 1], [0, 0, 4], [0, 0, 5], [0, 0, 0, 0], [0, 1, 1], [0, 0, 6], [0, 3, 0], [5, 0], [1, 3], [0, 0, 7], [0, 0, 8], [0, 0, 9], [6, 0], [0, 4, 0], [1, 4], [7, 0], [0, 1, 2], [2, 0, 0], [3, 1], [2, 2], [8, 0], [0, 5, 0], [1, 5], [1, 0, 1], [0, 2, 1], [9, 0], [0, 6, 0], [0, 0, 0, 1], [1, 6], [0, 7, 0]]\n",
                "TOPK = 10\n",
                "\n",
                "class MedusaBuffer:\n",
                "    def __init__(self, medusa_choice=mc_sim_7b_63):\n",
                "        self.medusa_choice = sorted(medusa_choice, key= lambda x: (len(x),x))\n",
                "        self.get_medusa_attn_mask()\n",
                "        self.get_medusa_position_ids()\n",
                "        self.get_retrieve_indices()\n",
                "        self.get_tree_indices()\n",
                "\n",
                "    @staticmethod\n",
                "    def pad_path(path, length, pad_value=-2):\n",
                "        return path + [pad_value] * (length - len(path))\n",
                "    \n",
                "    def plot_attn_mask(self):\n",
                "        plt.imshow(self.medusa_attn_mask)\n",
                "\n",
                "    def plot_tree(self):\n",
                "        plt.figure(figsize=(40, 20)) \n",
                "        paths = self.medusa_choice\n",
                "        G = nx.DiGraph()\n",
                "        for path in paths:\n",
                "            for i in range(len(path)):\n",
                "                if i == 0:\n",
                "                    parent = 'root'\n",
                "                else:\n",
                "                    parent = tuple(path[:i])\n",
                "                child = tuple(path[:i+1])\n",
                "                G.add_edge(parent, child)\n",
                "\n",
                "        # Use the Graphviz layout for drawing.\n",
                "        pos = nx.nx_agraph.graphviz_layout(G, prog='dot')\n",
                "        nx.draw(G, pos, with_labels=True, node_size=500, node_color=\"skyblue\", font_size=10, width=2, edge_color=\"gray\")\n",
                "        plt.show()\n",
                "\n",
                "    def to_dict(self, device=None):\n",
                "        buffer_dict ={\n",
                "            'medusa_attn_mask': self.medusa_attn_mask,\n",
                "            'medusa_position_ids': self.medusa_position_ids,\n",
                "            'tree_indices': self.tree_indices,\n",
                "            'retrieve_indices': self.retrieve_indices\n",
                "        }\n",
                "        if device is not None:\n",
                "            for k, v in buffer_dict.items():\n",
                "                buffer_dict[k] = v.to(device)\n",
                "        return buffer_dict\n",
                "\n",
                "    def get_medusa_attn_mask(self):\n",
                "        len_medusa = len(self.medusa_choice)+1\n",
                "        self.medusa_attn_mask = torch.eye(len_medusa,len_medusa)\n",
                "        self.medusa_attn_mask[:,0] = 1\n",
                "\n",
                "        for i in range(len(self.medusa_choice)):\n",
                "            if len(self.medusa_choice[i]) == 1:\n",
                "                continue\n",
                "\n",
                "            idx = self.medusa_choice.index(self.medusa_choice[i])\n",
                "            self.medusa_attn_mask[i+1, idx+1] = 1\n",
                "            for j in range(1, len(self.medusa_choice[i])):\n",
                "                idx = self.medusa_choice.index(self.medusa_choice[i][:-j])\n",
                "                self.medusa_attn_mask[i+1,idx+1] = 1\n",
                "    \n",
                "    def get_medusa_position_ids(self):\n",
                "        self.medusa_position_ids = torch.tensor([len(choice) for choice in self.medusa_choice])\n",
                "        self.medusa_position_ids = torch.cat([torch.zeros(1), self.medusa_position_ids])\n",
                "    \n",
                "    def get_tree_indices(self):\n",
                "        medusa_len = len(self.medusa_choice) + 1\n",
                "        depth_counts = torch.unique(self.medusa_position_ids[1:], return_counts=True)[1].tolist()\n",
                "        self.tree_indices = torch.zeros(medusa_len, dtype=torch.long)\n",
                "        self.tree_indices[0] = 0\n",
                "        start = 0\n",
                "        for i in range(len(depth_counts)):\n",
                "            for j in range(depth_counts[i]):\n",
                "                cur_medusa_choice = self.medusa_choice[start + j]\n",
                "                self.tree_indices[start + j + 1] = cur_medusa_choice[-1] + TOPK * i + 1\n",
                "            start += depth_counts[i]\n",
                "    \n",
                "    def get_retrieve_indices(self):\n",
                "        visited_paths = []\n",
                "        indices = []\n",
                "        for idx, row in enumerate(reversed(self.medusa_attn_mask[1:,1:])):\n",
                "            cur_medusa_choice = self.medusa_choice[-idx-1]\n",
                "            if cur_medusa_choice in visited_paths:\n",
                "                continue\n",
                "            else:\n",
                "                indices.append(torch.where(row == 1)[0].tolist())\n",
                "                visited_paths += [cur_medusa_choice[: c + 1] for c in range(len(cur_medusa_choice))]\n",
                "        max_length = max([len(x) for x in visited_paths])\n",
                "        self.retrieve_indices = [MedusaBuffer.pad_path(path, max_length) for path in indices]\n",
                "        self.retrieve_indices = torch.tensor(self.retrieve_indices, dtype=torch.long)\n",
                "        self.retrieve_indices = self.retrieve_indices + 1\n",
                "        self.retrieve_indices = torch.cat(\n",
                "            [\n",
                "                torch.zeros((self.retrieve_indices.shape[0], 1), dtype=torch.long),\n",
                "                self.retrieve_indices,\n",
                "            ],\n",
                "            dim=1,\n",
                "        )"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 71,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MedusaDecoding:\n",
                "    def __init__(self, model=None, tokenizer=None):\n",
                "        self.model = model\n",
                "        self.tokenizer = tokenizer\n",
                "        self.generate_medusa_buffer()\n",
                "\n",
                "    def generate_medusa_buffer(self, device=None):\n",
                "        self.medusa_buffer = MedusaBuffer().to_dict(device=device)\n",
                "\n",
                "    def top_p_nucleus_sampling(self, logits, temperature, p):\n",
                "        probs = torch.softmax(logits / temperature, dim=-1)\n",
                "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
                "        probs_cumsum = torch.cumsum(probs_sort, dim=-1)\n",
                "        valid_indices = probs_cumsum <= p\n",
                "        valid_indices[..., 0] = True\n",
                "        probs_sort[~valid_indices] = 0.0\n",
                "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True) + 1e-9)\n",
                "        probs_sort = torch.clamp(probs_sort, min=0.0, max=1.0)\n",
                "        next_token = torch.multinomial(probs_sort.view(-1, probs_sort.size(-1)), num_samples=1)\n",
                "        next_token = next_token.view(probs_sort.shape[:-1] + (1,))\n",
                "        next_token = torch.gather(probs_idx, -1, next_token)\n",
                "        return next_token\n",
                "\n",
                "    def generate_candidates(\n",
                "            self,\n",
                "            medusa_logits,\n",
                "            logits,\n",
                "            temperature=1,\n",
                "            p=1,\n",
                "    ):\n",
                "        candidates_logit = self.top_p_nucleus_sampling(logits[0,-1], temperature, p)\n",
                "        candidates_medusa_logits = torch.topk(medusa_logits[:, 0, -1], TOPK, dim=-1).indices\n",
                "        candidates = torch.cat([candidates_logit, candidates_medusa_logits.view(-1)], dim=-1)\n",
                "\n",
                "        # Map the combined candidates to the tree indices to get tree candidates.\n",
                "        tree_candidates = candidates[self.medusa_buffer['tree_indices']]\n",
                "\n",
                "        # Extend the tree candidates by appending a zero.\n",
                "        tree_candidates_ext = torch.cat(\n",
                "            [\n",
                "                tree_candidates,\n",
                "                torch.zeros((1), dtype=torch.long, device=tree_candidates.device),\n",
                "            ],\n",
                "            dim=0,\n",
                "        )\n",
                "\n",
                "        # Retrieve the cartesian candidates using the retrieve indices.\n",
                "        cart_candidates = tree_candidates_ext[self.medusa_buffer['retrieve_indices']]\n",
                "        # Unsqueeze the tree candidates for dimension consistency.\n",
                "        tree_candidates = tree_candidates.unsqueeze(0)\n",
                "        return cart_candidates, tree_candidates\n",
                "\n",
                "    def tree_decoding(\n",
                "            self,\n",
                "            input_ids,\n",
                "            model = None,\n",
                "            tree_candidates = None,\n",
                "            past_key_values = None,\n",
                "    ):\n",
                "        \n",
                "        # Compute new position IDs by adding the Medusa position IDs to the length of the input sequence.\n",
                "        position_ids = self.medusa_buffer['medusa_position_ids'] + input_ids.shape[1]\n",
                "\n",
                "        # # Use the model to decode the tree candidates.\n",
                "        # # The model is expected to return logits for the Medusa structure, original logits, and possibly other outputs.\n",
                "        # tree_medusa_logits, outputs, tree_logits = model(\n",
                "        #     tree_candidates,\n",
                "        #     output_orig=True,\n",
                "        #     past_key_values=past_key_values,\n",
                "        #     position_ids=position_ids,\n",
                "        #     medusa_forward=True,\n",
                "        # )\n",
                "        # currently randomly generating for illustration\n",
                "        tree_logits = torch.randn(1, 64, 32000)\n",
                "        tree_medusa_logits = torch.randn(5, 1, 64, 32000)\n",
                "\n",
                "        # Reorder the obtained logits based on the retrieve_indices to ensure consistency with some reference ordering.\n",
                "        logits = tree_logits[0, self.medusa_buffer['retrieve_indices']]\n",
                "        medusa_logits = tree_medusa_logits[:, 0, self.medusa_buffer['retrieve_indices']]\n",
                "        return medusa_logits, logits, None\n",
                "    \n",
                "    def evalulate_posterior(self, logits, candidates, temperature=1, p=0.8):\n",
                "        logits = self.top_p_nucleus_sampling(logits, temperature, p)\n",
                "        posterior_mask = (\n",
                "                candidates[:, 1:] == logits.squeeze(-1)[:, :-1]\n",
                "        ).int()\n",
                "        candidates_accept_length = (torch.cumprod(posterior_mask, dim=1)).sum(dim=1)\n",
                "        accept_length = candidates_accept_length.max()\n",
                "        # Choose the best candidate\n",
                "        if accept_length == 0:\n",
                "            # Default to the first candidate if none are accepted\n",
                "            best_candidate = torch.tensor(0, dtype=torch.long, device=candidates.device)\n",
                "        else:\n",
                "            best_candidate = torch.argmax(candidates_accept_length).to(torch.long)\n",
                "        return best_candidate, accept_length\n",
                "\n",
                "    def update_inference_inputs(self, best_candidate, accept_length, input_ids, cart_candidates, logits, medusa_logits, new_token, past_key_values_data=None, current_length_data=None):\n",
                "        # Calculate the starting position for new tokens based on the previous input length\n",
                "        prev_input_len = input_ids.shape[1]\n",
                "        # Map the best candidate indices to the original indices in the sequence\n",
                "        select_indices = (\n",
                "            self.medusa_buffer['retrieve_indices'][best_candidate, : accept_length + 1] + prev_input_len\n",
                "        )     \n",
                "        # Append the tokens from the best candidate to the input sequence\n",
                "        input_ids = torch.cat(\n",
                "            [input_ids, cart_candidates[None, best_candidate, : accept_length + 1]], dim=-1\n",
                "        )\n",
                "\n",
                "        # # Update the past key values based on the selected tokens\n",
                "        # tgt = past_key_values_data[..., select_indices, :]\n",
                "        # dst = past_key_values_data[..., prev_input_len : prev_input_len + tgt.shape[-2], :]\n",
                "        # dst.copy_(tgt, non_blocking=True)\n",
                "        # current_length_data.fill_(prev_input_len + tgt.shape[-2])\n",
                "\n",
                "        # Extract logits and medusa logits for the accepted tokens\n",
                "        logits = logits[None, best_candidate, accept_length : accept_length + 1]\n",
                "        medusa_logits = medusa_logits[\n",
                "            :, None, best_candidate, accept_length : accept_length + 1\n",
                "        ]\n",
                "        # Update the new token counter\n",
                "        new_token += accept_length + 1\n",
                "\n",
                "        return input_ids, logits, medusa_logits, new_token"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "medusa_logits = torch.randn(5,1,10,4096)\n",
                "logits = torch.randn(1,10,4096)\n",
                "input_ids = torch.randint(0, 32000, (1, 10))\n",
                "\n",
                "decoder = MedusaDecoding()\n",
                "cart_candidates, tree_candidates = decoder.generate_candidates(medusa_logits, logits)\n",
                "medusa_logits, logits, outputs = decoder.tree_decoding(input_ids)\n",
                "best_candidate, accept_length = decoder.evalulate_posterior(logits, cart_candidates)\n",
                "input_ids, logits, medusa_logits, new_token = decoder.update_inference_inputs(best_candidate, accept_length, input_ids, cart_candidates, logits, medusa_logits, 0)\n",
                "\n",
                "print(medusa_logits.shape, logits.shape, input_ids.shape, new_token)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
